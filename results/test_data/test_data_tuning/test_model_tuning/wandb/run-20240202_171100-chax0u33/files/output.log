D:\04_GitRepos\addmo-extra\core\model_tuning\hyperparameter_tuning\hyperparameter_tuner.py:53: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.
  wandbc = [WeightsAndBiasesCallback(wandb_kwargs=wandb_kwargs)]
[I 2024-02-02 17:11:05,664] A new study created in memory with name: no-name-a9e5cd17-404f-480e-85ce-b49fa7d3c34b
[I 2024-02-02 17:11:11,388] Trial 0 finished with value: 0.8279729410938789 and parameters: {'n_layers': 3, 'n_units_l0': 37, 'n_units_l1': 94, 'n_units_l2': 81}. Best is trial 0 with value: 0.8279729410938789.
[I 2024-02-02 17:11:16,094] Trial 1 finished with value: 0.8534364241746394 and parameters: {'n_layers': 2, 'n_units_l0': 18, 'n_units_l1': 83}. Best is trial 1 with value: 0.8534364241746394.
wandb: WARNING Step only supports monotonically increasing values, use define_metric to set a custom x axis. For details see: https://wandb.me/define-metric
wandb: WARNING (User provided step: 0 is less than current step: 3. Dropping entry: {'n_layers': 3, 'n_units_l0': 37, 'n_units_l1': 94, 'n_units_l2': 81, 'value': 0.8279729410938789, '_timestamp': 1706890271.3889136}).
wandb: WARNING (User provided step: 1 is less than current step: 4. Dropping entry: {'n_layers': 2, 'n_units_l0': 18, 'n_units_l1': 83, 'value': 0.8534364241746394, '_timestamp': 1706890276.0947917}).
[I 2024-02-02 17:11:22,939] Trial 2 finished with value: 0.81736277144785 and parameters: {'n_layers': 3, 'n_units_l0': 62, 'n_units_l1': 79, 'n_units_l2': 65}. Best is trial 1 with value: 0.8534364241746394.
[I 2024-02-02 17:11:27,950] Trial 3 finished with value: 0.8200789230731335 and parameters: {'n_layers': 2, 'n_units_l0': 54, 'n_units_l1': 75}. Best is trial 1 with value: 0.8534364241746394.
[I 2024-02-02 17:11:32,253] Trial 4 finished with value: 0.8286993566041657 and parameters: {'n_layers': 2, 'n_units_l0': 79, 'n_units_l1': 24}. Best is trial 1 with value: 0.8534364241746394.
wandb: WARNING (User provided step: 2 is less than current step: 5. Dropping entry: {'n_layers': 3, 'n_units_l0': 62, 'n_units_l1': 79, 'n_units_l2': 65, 'value': 0.81736277144785, '_timestamp': 1706890282.9402175}).
wandb: WARNING (User provided step: 3 is less than current step: 6. Dropping entry: {'n_layers': 2, 'n_units_l0': 54, 'n_units_l1': 75, 'value': 0.8200789230731335, '_timestamp': 1706890287.950067}).
wandb: WARNING (User provided step: 4 is less than current step: 7. Dropping entry: {'n_layers': 2, 'n_units_l0': 79, 'n_units_l1': 24, 'value': 0.8286993566041657, '_timestamp': 1706890292.2549293}).
[I 2024-02-02 17:11:36,943] Trial 5 finished with value: 0.8118836558219215 and parameters: {'n_layers': 2, 'n_units_l0': 68, 'n_units_l1': 78}. Best is trial 1 with value: 0.8534364241746394.
[I 2024-02-02 17:11:41,309] Trial 6 finished with value: 0.8360731816205798 and parameters: {'n_layers': 1, 'n_units_l0': 84}. Best is trial 1 with value: 0.8534364241746394.
[I 2024-02-02 17:11:45,121] Trial 7 finished with value: 0.8144171481390821 and parameters: {'n_layers': 1, 'n_units_l0': 92}. Best is trial 1 with value: 0.8534364241746394.
wandb: WARNING (User provided step: 5 is less than current step: 8. Dropping entry: {'n_layers': 2, 'n_units_l0': 68, 'n_units_l1': 78, 'value': 0.8118836558219215, '_timestamp': 1706890296.944103}).
wandb: WARNING (User provided step: 6 is less than current step: 9. Dropping entry: {'n_layers': 1, 'n_units_l0': 84, 'value': 0.8360731816205798, '_timestamp': 1706890301.3100276}).
wandb: WARNING (User provided step: 7 is less than current step: 10. Dropping entry: {'n_layers': 1, 'n_units_l0': 92, 'value': 0.8144171481390821, '_timestamp': 1706890305.1210577}).
[I 2024-02-02 17:11:48,576] Trial 8 finished with value: 0.8425849248872112 and parameters: {'n_layers': 3, 'n_units_l0': 82, 'n_units_l1': 22, 'n_units_l2': 44}. Best is trial 1 with value: 0.8534364241746394.
[I 2024-02-02 17:11:51,876] Trial 9 finished with value: 0.8170006163100659 and parameters: {'n_layers': 1, 'n_units_l0': 54}. Best is trial 1 with value: 0.8534364241746394.
Finished